{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Index Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-index'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "import datetime\n",
    "\n",
    "container_name = os.getenv(\"storage_container\")\n",
    "storage_base_url = os.getenv(\"storage_base_url\")\n",
    "connection_string = os.getenv(\"storage_connection_string\")\n",
    "\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "embedding_model = os.getenv(\"embeddingModel\")\n",
    "chat_model = os.getenv(\"chatModel\")\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "index_name = os.getenv(\"SEARCH_INDEX_NAME\")\n",
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection name: ai-ericssonlearningpathhub131886796645_aoai\n",
      "connection name: ai-ericssonlearningpathhub131886796645\n",
      "connection name: searchericssonlearningpath\n",
      "connection name: bingericssonlearningpath\n",
      "connection name: aiericssonlearningpathwesteurope\n",
      "connection name: ericsson-learning-path-project/demo_data\n",
      "connection name: ericsson-learning-path-project/demodata\n",
      "connection name: ericsson-learning-path-project/product_data\n",
      "connection name: ericsson-learning-path-project/customer_data\n",
      "connection name: ericsson-learning-path-project/workspaceartifactstore\n",
      "connection name: ericsson-learning-path-project/workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "\n",
    "# Creating a project client\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "for connection in project_client.connections.list():\n",
    "    print(f\"connection name: {connection.name}\")\n",
    "\n",
    "# Creating an AI search connection\n",
    "search_connection = project_client.connections.get(\n",
    "    connection_name=os.getenv(\"SEARCH_CONNECTION_NAME\"),\n",
    "    include_credentials=True)\n",
    "\n",
    "# Creating an OpenAI connection\n",
    "oai_connection = project_client.connections.get(\n",
    "    connection_name=os.getenv(\"OAI_CONNECTION_NAME\"),\n",
    "    include_credentials=True)\n",
    "\n",
    "ai_connection = project_client.connections.get(\n",
    "    connection_name=os.getenv(\"AI_CONNECTION_NAME\"),\n",
    "    include_credentials=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_client = project_client.inference.get_embeddings_client()\n",
    "\n",
    "def get_embedding(text):\n",
    "    # get an embedding for the text using the project's default model inferencing endpoint\n",
    "    embedding = embedding_client.embed(\n",
    "        input=text,\n",
    "        dimensions=1536,\n",
    "        model=embedding_model,\n",
    "    )\n",
    "    return embedding.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch\n",
    ")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_connection.endpoint_url,\n",
    "    credential=AzureKeyCredential(key=search_connection.key)\n",
    ")\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True, sortable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"last_update\", type=SearchFieldDataType.DateTimeOffset, filterable=True),\n",
    "    SimpleField(name=\"url\", type=SearchFieldDataType.String),\n",
    "    SearchField(\n",
    "        name=\"text_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1536,\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    )\n",
    "]\n",
    "\n",
    "# Adding vector search settings\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            vectorizer_name=\"myVectorizer\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"myVectorizer\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=oai_connection.endpoint_url, #azure_openai_endpoint,\n",
    "                deployment_name=embedding_model,\n",
    "                model_name=embedding_model,\n",
    "                api_key=oai_connection.key,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test-index created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndex\n",
    "\n",
    "# Create the search index\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search\n",
    ")\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Pipeline**\n",
    "- **Read data from storage account**\n",
    "- **Use Document Intelligence to crack PDF**\n",
    "    - **Extract text**\n",
    "    - **Extract images and write to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "\n",
    "def initialize_blob_service_client(connection_string, container_name):\n",
    "    # Initialize the BlobServiceClient and returns the container client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    return container_client\n",
    "\n",
    "def initialize_document_intelligence_client(ai_connection):\n",
    "    # Initialize the Document Intelligence client\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=ai_connection.endpoint_url,\n",
    "        credential=AzureKeyCredential(ai_connection.key)\n",
    "    )\n",
    "    return document_intelligence_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob_content(blob_client):\n",
    "    # Download the blob's content\n",
    "    try:\n",
    "        download_stream = blob_client.download_blob()\n",
    "        blob_content = download_stream.readall()\n",
    "        return blob_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading blob: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_document(document_intelligence_client, blob_content):\n",
    "    # Analyze the document using the Document Intelligence client\n",
    "    from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        model_id=\"prebuilt-layout\",\n",
    "        analyze_request=blob_content,\n",
    "        content_type=\"application/octet-stream\",  # Adjust based on your document type\n",
    "    )\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Document Cracking Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1706.03762v7.pdf: 100%|██████████| 1/1 [00:09<00:00,  9.20s/blob, Status=Finished]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_process_data_pipeline():\n",
    "    documents = []\n",
    "\n",
    "    # Initialize the BlobServiceClient & Document Intelligence client\n",
    "    container_client = initialize_blob_service_client(connection_string, container_name)\n",
    "    document_intelligence_client = initialize_document_intelligence_client(ai_connection)\n",
    "\n",
    "    # List all blobs in the container\n",
    "    blob_list = list(container_client.list_blobs())  # Convert generator to list to get total count\n",
    "    total_blobs = len(blob_list)  # Total number of blobs\n",
    "\n",
    "    if total_blobs == 0:\n",
    "        print(\"No blobs found in the container.\")\n",
    "    else:\n",
    "        with tqdm(total=total_blobs, desc=\"Processing Blobs\", unit=\"blob\") as pbar:\n",
    "            for blob in blob_list:\n",
    "                blob_name = blob.name\n",
    "\n",
    "                # Update the progress bar's description to show the current blob\n",
    "                pbar.set_description(f\"Processing {blob_name}\")\n",
    "\n",
    "                # Download the blob's content\n",
    "                blob_content = download_blob_content(blob_client = container_client.get_blob_client(blob_name))\n",
    "\n",
    "                if blob_content is None:\n",
    "                    continue # Skip to the next blob if download failed\n",
    "\n",
    "                # Analyze the document using Document Intelligence\n",
    "                data = analyze_document(document_intelligence_client, blob_content)\n",
    "\n",
    "                if data is None:\n",
    "                    continue # Skip to the next blob if analysis failed\n",
    "\n",
    "                documents.append({\n",
    "                    \"filename\": blob_name,\n",
    "                    \"data\": data,\n",
    "                    \"url\": f\"{storage_base_url}/{container_name}/{blob_name}\"\n",
    "                })\n",
    "\n",
    "                pbar.update(1)\n",
    "            pbar.set_postfix({\"Status\": \"Finished\"})\n",
    "    return documents\n",
    "\n",
    "documents_raw = run_process_data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Chunks + Add Metadata**\n",
    "- **chunk id**\n",
    "- **last update**\n",
    "- **number of tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filename', 'data', 'url'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_raw[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def get_sweden_time():\n",
    "    # Define the timezone for Sweden\n",
    "    sweden_tz = pytz.timezone('Europe/Stockholm')\n",
    "    utc_now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    sweden_time = utc_now.astimezone(sweden_tz)\n",
    "    return sweden_time.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(raw_doc):\n",
    "    document_chunks = []\n",
    "    current_title = None\n",
    "\n",
    "    data = raw_doc[\"data\"]\n",
    "\n",
    "    # Loop through paragraphs to structure the main content\n",
    "    for paragraph in data.paragraphs:\n",
    "        # Extract page number from boundingRegions\n",
    "        if paragraph.role == \"title\":\n",
    "            # Update the current title but do not add an entry to document_chunks\n",
    "            current_title = paragraph.content\n",
    "        elif paragraph.role == \"sectionHeading\":\n",
    "            # Start a new entry for the section heading with the current title\n",
    "            document_chunks.append({\n",
    "                \"title\": current_title,\n",
    "                \"content\": \"\",\n",
    "            })\n",
    "        else:\n",
    "            # Add content to the last entry, updating page_end as needed\n",
    "            if document_chunks:\n",
    "                document_chunks[-1][\"content\"] += \" \" + paragraph.content\n",
    "    return document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "def create_chunks(content, chunk_size=1025, chunk_overlap=128):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    # This list will hold our final dictionaries with updated content.\n",
    "    splits = []\n",
    "\n",
    "    # Iterate through original documents and create chunks\n",
    "    for doc in content:\n",
    "        text = doc.get(\"content\", \"\")\n",
    "        \n",
    "        # Update the dictionary to use `page_content` instead of `content`\n",
    "        doc[\"content\"] = text\n",
    "        doc.pop(\"content\", None)  # Remove the old `content` key if it exists\n",
    "\n",
    "        chunks = text_splitter.create_documents([text])\n",
    "        if len(chunks) < 1:\n",
    "            splits.append(doc)\n",
    "        else:\n",
    "            for chunk in chunks:\n",
    "                new_doc = doc.copy()\n",
    "                new_doc[\"content\"] = chunk.page_content  # Assign the splitted text\n",
    "                splits.append(new_doc)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Merged function that combines format_documents and convert_document\n",
    "def process_documents(documents_raw, get_embedding_fn):\n",
    "    processed_documents = []\n",
    "    namespace = uuid.UUID(\"6ba7b810-9dad-11d1-80b4-00c04fd430c8\")\n",
    "    \n",
    "    # Process each document with progress tracking\n",
    "    for doc in tqdm(documents_raw, desc=\"Processing documents\"):\n",
    "        url = doc[\"url\"]  # URL for the document\n",
    "        filename = doc[\"filename\"]  # Filename of the document\n",
    "\n",
    "        # Split the raw document based on title and section headings\n",
    "        splits = create_splits(doc)\n",
    "        # Create chunks from the splits if needed\n",
    "        chunks = create_chunks(splits, chunk_size=1024, chunk_overlap=128)\n",
    "\n",
    "        # Process each chunk with embeddings in a single pass\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_name = f\"{filename}_chunk_{i}\"\n",
    "            \n",
    "            # Generate a unique ID for this chunk\n",
    "            chunk_id = str(uuid.uuid5(namespace, chunk_name))\n",
    "            \n",
    "            # Get the content from the chunk\n",
    "            content = chunk.get(\"content\", \"\") if isinstance(chunk, dict) else chunk.page_content\n",
    "            \n",
    "            # Generate embedding vector for this content\n",
    "            text_vector = get_embedding_fn(content) if content else []\n",
    "            \n",
    "            # Build the complete document with all required fields\n",
    "            processed_documents.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"title\": chunk_name,\n",
    "                \"content\": content,\n",
    "                \"last_update\": get_sweden_time(),\n",
    "                \"url\": url,\n",
    "                \"text_vector\": text_vector\n",
    "            })\n",
    "            \n",
    "    return processed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process and index data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1/1 [00:09<00:00,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed in 9.13 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Process documents and generate embeddings in one step\n",
    "data_final = process_documents(documents_raw, get_embedding)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Processing completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Push to index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "def push_to_index(data, search_connection, index_name=index_name):\n",
    "    search_client = SearchClient(\n",
    "        index_name=index_name,\n",
    "        endpoint=search_connection.endpoint_url,\n",
    "        credential=AzureKeyCredential(key=search_connection.key)\n",
    "    )\n",
    "    search_client.upload_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 5/5 [00:03<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches pushed in 3.04 seconds to test-index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Group the header based chunks into batches\n",
    "batch_size = 5\n",
    "total_chunks = len(data_final)\n",
    "num_batches = (total_chunks + batch_size - 1) // batch_size\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# Process each batch\n",
    "for i, batch_num in enumerate(tqdm(range(num_batches), desc=\"Processing Batches\")):\n",
    "    batch_start_time = time.time()\n",
    "    start = batch_num * batch_size\n",
    "    batch = data_final[start:start + batch_size]\n",
    "\n",
    "    # Push the documents to the index\n",
    "    push_to_index(\n",
    "        data=batch,\n",
    "        search_connection=search_connection,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "overall_end_time = time.time()\n",
    "elapsed_overall_time = overall_end_time - overall_start_time\n",
    "print(f\"All batches pushed in {elapsed_overall_time:.2f} seconds to {index_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ericsson-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
