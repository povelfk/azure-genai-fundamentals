{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "from opentelemetry import trace\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# create a project client using environment variables loaded from the .env file\n",
    "project = AIProjectClient.from_connection_string(\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"], credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# create a vector embeddings client that will be used to generate vector embeddings\n",
    "chat = project.inference.get_chat_completions_client()\n",
    "embeddings = project.inference.get_embeddings_client()\n",
    "\n",
    "# use the project client to get the default search connection\n",
    "search_connection = project.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_AI_SEARCH, include_credentials=True\n",
    ")\n",
    "\n",
    "# Create a search index client using the search connection\n",
    "# This client will be used to create and delete search indexes\n",
    "search_client = SearchClient(\n",
    "    index_name=os.environ[\"SEARCH_INDEX_NAME\"],\n",
    "    endpoint=search_connection.endpoint_url,\n",
    "    credential=AzureKeyCredential(key=search_connection.key),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RETRIEVAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTENT SYSTEM MESSAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import UserMessage, SystemMessage\n",
    "\n",
    "# Define your INTENT_SYSTEM_PROMPT correctly with escaped braces\n",
    "INTENT_SYSTEM_PROMPT = \"\"\"\n",
    "    # Intent Mapping System\n",
    "\n",
    "    Your task is to understand the user's query and map it to a search intent.\n",
    "    \n",
    "    For example, if a user asks about \"attention mechanisms in transformers\", \n",
    "    create a search query like \"attention mechanism transformer architecture neural networks\".\n",
    "    \n",
    "    Avoid phrases like \"I want\" or \"tell me about\". Just provide keywords.\n",
    "    \n",
    "    The user's conversation history is:\n",
    "    {conversation_history}\n",
    "    \n",
    "    Return only the search query, nothing else. Use the format: \n",
    "    {{\"intent\": \"your search query here\"}}\n",
    "\"\"\"\n",
    "\n",
    "# Then fix your get_intent_system_message function to properly escape the curly braces in the output\n",
    "def get_intent_system_message(conversation_history):\n",
    "    return SystemMessage(INTENT_SYSTEM_PROMPT.format(conversation_history=conversation_history)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RETRIEVE DOCUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "import json\n",
    "\n",
    "@tracer.start_as_current_span(name=\"get_product_documents\")\n",
    "def get_product_documents(messages: list, top: int=3) -> dict:\n",
    "    intent_query_response = chat.complete(\n",
    "        model=os.environ[\"chatModel\"],\n",
    "        messages=[get_intent_system_message(messages)]\n",
    "    )\n",
    "\n",
    "    enhanced_search_query = json.loads(intent_query_response.choices[0].message.content)[\"intent\"]\n",
    "    \n",
    "    embedding = embeddings.embed(model=os.environ[\"embeddingModel\"], input=enhanced_search_query)\n",
    "    search_vector = embedding.data[0].embedding\n",
    "    vector_query = VectorizedQuery(vector=search_vector, k_nearest_neighbors=50, fields=\"text_vector\")\n",
    "\n",
    "    search_results = search_client.search(\n",
    "        search_text=enhanced_search_query,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"id\", \"content\", \"title\", \"url\"],\n",
    "        top=top,\n",
    "    )\n",
    "\n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": result[\"id\"],\n",
    "            \"content\": result[\"content\"],\n",
    "            \"title\": result[\"title\"],\n",
    "            \"url\": result[\"url\"],\n",
    "        }\n",
    "        for result in search_results\n",
    "    ]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Completion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG SYSTEM MESSAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that provides accurate information based on the retrieved context.\n",
    "\n",
    "### Retrieved Context:\n",
    "{retrieved_context}\n",
    "\n",
    "### Instructions:\n",
    "1. Answer questions based on the retrieved context above\n",
    "2. If the context doesn't contain the information needed, acknowledge the limitation\n",
    "3. Do not make up information that is not supported by the context\n",
    "4. Keep responses concise and focused on the user's question\n",
    "5. Format your answers using Markdown when appropriate\n",
    "6. When quoting directly from the context, use quotation marks\n",
    "\n",
    "Remember: Only use information from the retrieved context to answer questions.\n",
    "\"\"\"\n",
    "\n",
    "def get_completion_system_message(retrieved_context):\n",
    "    return SystemMessage(SYSTEM_PROMPT.format(retrieved_context=retrieved_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.start_as_current_span(name=\"chat_with_attentionIsAllYouNeed\")\n",
    "def chat_with_products(messages: list) -> dict:\n",
    "    documents = get_product_documents(messages)\n",
    "    \n",
    "    # Create the system message\n",
    "    system_message = get_completion_system_message(documents)\n",
    "\n",
    "    # Format messages properly for the API\n",
    "    formatted_messages = [system_message]\n",
    "    \n",
    "    # Add user messages\n",
    "    for message in messages:\n",
    "        print(f\"message: {message}\")\n",
    "        formatted_messages.append(UserMessage(message[\"content\"]))\n",
    "    \n",
    "    response = chat.complete(\n",
    "        model=os.environ[\"chatModel\"],\n",
    "        messages=formatted_messages\n",
    "    )\n",
    "    # Return a chat protocol compliant response\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.core.settings import settings\n",
    "\n",
    "def enable_telemetry(project):\n",
    "    AIInferenceInstrumentor().instrument()\n",
    "    settings.tracing_implementation = \"opentelemetry\"\n",
    "    application_insights_connection_string = project.telemetry.get_connection_string()\n",
    "    configure_azure_monitor(connection_string=application_insights_connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message: {'role': 'user', 'content': 'how does attention relate to feed forward networks?'}\n"
     ]
    }
   ],
   "source": [
    "# from config import enable_telemetry\n",
    "enable_telemetry(project)\n",
    "\n",
    "user_message = \"how does attention relate to feed forward networks?\"\n",
    "response = chat_with_products(messages=[{\"role\": \"user\", \"content\": user_message}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the Transformer model, attention mechanisms and feed forward networks are both integral parts of the architecture used in the encoder and decoder. The Transformer leverages self-attention to compute representations of input and output sequences without relying on sequential RNNs or convolution.\n",
       "\n",
       "Based on the context, the relationship between attention and feed forward networks in the Transformer can be described as follows:\n",
       "\n",
       "1. **Attention Mechanisms**:\n",
       "   - Attention mechanisms are used to model dependencies between different positions of the input and output sequences. They do this without regard to the distance between the positions, enabling parallel computation.\n",
       "\n",
       "2. **Feed Forward Networks**:\n",
       "   - After applying attention, the output is passed through point-wise, fully connected feed forward layers. These layers are responsible for transforming the attention-weighted positions.\n",
       "\n",
       "In the detailed architecture (Figure 1 from the context), self-attention and feed forward layers are stacked together:\n",
       "\n",
       "- **Encoder and Decoder**:\n",
       "  - Both the encoder and decoder use \"stacked self-attention and point-wise, fully connected layers.\" This means that within each encoder and decoder layer, there is an attention computation followed by a feed forward network.\n",
       "\n",
       "To summarize, attention mechanisms relate different positions within a sequence, creating a context-aware representation, while feed forward networks further process these representations to refine the output. Together, they work in tandem within the layers of the Transformer model.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ericsson-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
