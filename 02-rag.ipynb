{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retrieval-Augmented Generation (RAG) with Azure AI Foundry**\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to implement Retrieval-Augmented Generation (RAG) using Azure AI Foundry services. You'll learn how to enhance AI model responses with relevant information retrieved from your own data sources, improving accuracy and context-awareness of the responses.\n",
    "\n",
    "## What is RAG?\n",
    "Retrieval-Augmented Generation combines the strengths of two approaches:\n",
    "\n",
    "1. **Retrieval**: Finding relevant information from a knowledge base or vector store\n",
    "2. **Generation**: Using this retrieved context to generate accurate, informed responses\n",
    "\n",
    "This hybrid approach helps solve several limitations of standalone generative models:\n",
    "- Provides access to specialized knowledge not in the model's training data\n",
    "- Reduces hallucinations by grounding responses in factual information\n",
    "- Allows for real-time access to updated information\n",
    "- Makes citation and attribution of sources possible\n",
    "\n",
    "## Key Components in This Notebook\n",
    "\n",
    "### **1. Intent-Based Query Processing**\n",
    "- Converting user queries into optimized search queries\n",
    "- Enhancing search relevance through intent understanding\n",
    "\n",
    "### **2. Vector Search**\n",
    "- Using embeddings to find semantically similar content\n",
    "- Combining traditional keyword search with vector similarity\n",
    "\n",
    "### **3. Context-Aware Completion**\n",
    "- Providing retrieved documents as context to the language model\n",
    "- Formatting responses with proper citations\n",
    "\n",
    "### **4. Telemetry Integration**\n",
    "- Tracking query performance with Azure Monitor\n",
    "- Using OpenTelemetry for observability\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up a complete RAG pipeline using Azure AI Foundry\n",
    "- Implement intelligent query formulation\n",
    "- Configure vector search for semantic retrieval\n",
    "- Provide grounded, well-cited responses from an AI model\n",
    "- Monitor and analyze RAG system performance\n",
    "\n",
    "## Prerequisites\n",
    "- An Azure account with access to Azure AI Foundry\n",
    "- An active Azure AI Search index (created in the 01-create-index.ipynb notebook)\n",
    "- Appropriate environment variables in a `.env` file:\n",
    "  - `PROJECT_CONNECTION_STRING`: Connection string for your Azure AI Foundry project\n",
    "  - `SEARCH_INDEX_NAME`: Name of your Azure AI Search index\n",
    "  - `chatModel`: The model to use for chat completions (e.g., GPT-4)\n",
    "  - `embeddingModel`: The model to use for embeddings (e.g., text-embedding-ada-002)\n",
    "  - `APPLICATIONINSIGHTS_CONNECTION_STRING`: Connection string for telemetry\n",
    "\n",
    "Follow along step by step to build your own RAG solution with Azure AI Foundry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "from opentelemetry import trace\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# create a project client using environment variables loaded from the .env file\n",
    "project = AIProjectClient.from_connection_string(\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"], credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# create a vector embeddings client that will be used to generate vector embeddings\n",
    "chat = project.inference.get_chat_completions_client()\n",
    "embeddings = project.inference.get_embeddings_client()\n",
    "\n",
    "# use the project client to get the default search connection\n",
    "search_connection = project.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_AI_SEARCH, include_credentials=True\n",
    ")\n",
    "\n",
    "# Create a search index client using the search connection\n",
    "# This client will be used to create and delete search indexes\n",
    "search_client = SearchClient(\n",
    "    index_name=os.environ[\"SEARCH_INDEX_NAME\"],\n",
    "    endpoint=search_connection.endpoint_url,\n",
    "    credential=AzureKeyCredential(key=search_connection.key),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RETRIEVAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTENT SYSTEM MESSAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import UserMessage, SystemMessage\n",
    "\n",
    "# Define your INTENT_SYSTEM_PROMPT correctly with escaped braces\n",
    "INTENT_SYSTEM_PROMPT = \"\"\"\n",
    "    # Intent Mapping System\n",
    "\n",
    "    Your task is to understand the user's query and map it to a search intent.\n",
    "    \n",
    "    For example, if a user asks about \"attention mechanisms in transformers\", \n",
    "    create a search query like \"attention mechanism transformer architecture neural networks\".\n",
    "    \n",
    "    Avoid phrases like \"I want\" or \"tell me about\". Just provide keywords.\n",
    "    \n",
    "    The user's conversation history is:\n",
    "    {conversation_history}\n",
    "    \n",
    "    Return only the search query, nothing else. Use the format: \n",
    "    {{\"intent\": \"your search query here\"}}\n",
    "\"\"\"\n",
    "\n",
    "# Then fix your get_intent_system_message function to properly escape the curly braces in the output\n",
    "def get_intent_system_message(conversation_history):\n",
    "    return SystemMessage(INTENT_SYSTEM_PROMPT.format(conversation_history=conversation_history)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RETRIEVE DOCUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "import json\n",
    "\n",
    "@tracer.start_as_current_span(name=\"get_documents\")\n",
    "def get_documents(messages: list, top: int=3) -> dict:\n",
    "    intent_query_response = chat.complete(\n",
    "        model=os.environ[\"chatModel\"],\n",
    "        messages=[get_intent_system_message(messages)]\n",
    "    )\n",
    "\n",
    "    enhanced_search_query = json.loads(intent_query_response.choices[0].message.content)[\"intent\"]\n",
    "    \n",
    "    embedding = embeddings.embed(model=os.environ[\"embeddingModel\"], input=enhanced_search_query)\n",
    "    search_vector = embedding.data[0].embedding\n",
    "    vector_query = VectorizedQuery(vector=search_vector, k_nearest_neighbors=50, fields=\"text_vector\")\n",
    "\n",
    "    search_results = search_client.search(\n",
    "        search_text=enhanced_search_query,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"id\", \"content\", \"title\", \"url\"],\n",
    "        top=top,\n",
    "    )\n",
    "\n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": result[\"id\"],\n",
    "            \"content\": result[\"content\"],\n",
    "            \"title\": result[\"title\"],\n",
    "            \"url\": result[\"url\"],\n",
    "        }\n",
    "        for result in search_results\n",
    "    ]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Completion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG SYSTEM MESSAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that provides accurate information based on the retrieved context.\n",
    "\n",
    "### Retrieved Context:\n",
    "{retrieved_context}\n",
    "\n",
    "### Instructions:\n",
    "1. Answer questions based on the retrieved context above.\n",
    "2. If the context doesn't contain the information needed, acknowledge the limitation.\n",
    "3. Do not make up information that is not supported by the context.\n",
    "4. Keep responses concise and focused on the user's question.\n",
    "5. Format your answers using Markdown when appropriate (so we can render them using from IPython.display import display, Markdown).\n",
    "    5a. pay extra attention on formatting code blocks, lists, tables, and mathematical equations.\n",
    "6. When quoting directly from the context, use quotation marks.\n",
    "7. In terms of citation style please follow the Institute for Electrical and Electronics Engineers (IEEE):\n",
    "    7a. In-text citations should be in square brackets, e.g. [1], [2], etc.\n",
    "    7b. The reference list should be numbered in the order in which they appear in the text (make them clickable).\n",
    "    7c. Use the following format for references: [1] Author(s), \"Title,\" Journal, vol. X, no. Y, pp. Z-Z, Year. [Online]. Available: URL\n",
    "\n",
    "Remember: Only use information from the retrieved context to answer questions.\n",
    "\"\"\"\n",
    "\n",
    "def get_completion_system_message(retrieved_context):\n",
    "    return SystemMessage(SYSTEM_PROMPT.format(retrieved_context=retrieved_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.start_as_current_span(name=\"chat_with_attentionIsAllYouNeed\")\n",
    "def chat_with_documents(messages: list) -> dict:\n",
    "    documents = get_documents(messages)\n",
    "\n",
    "    # Create the system message\n",
    "    system_message = get_completion_system_message(documents)\n",
    "\n",
    "    # Format messages properly for the API\n",
    "    formatted_messages = [system_message]\n",
    "\n",
    "    # Add user messages\n",
    "    for message in messages:\n",
    "        print(f\"message: {message}\")\n",
    "        formatted_messages.append(UserMessage(message[\"content\"]))\n",
    "\n",
    "    response = chat.complete(\n",
    "        model=os.environ[\"chatModel\"],\n",
    "        messages=formatted_messages,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "\n",
    "    # Return a chat protocol compliant response\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.core.settings import settings\n",
    "\n",
    "def enable_telemetry(project):\n",
    "    AIInferenceInstrumentor().instrument()\n",
    "    settings.tracing_implementation = \"opentelemetry\"\n",
    "    application_insights_connection_string = project.telemetry.get_connection_string()\n",
    "    configure_azure_monitor(connection_string=application_insights_connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message: {'role': 'user', 'content': 'how does attention relate to feed forward networks?'}\n"
     ]
    }
   ],
   "source": [
    "# from config import enable_telemetry\n",
    "enable_telemetry(project)\n",
    "\n",
    "user_message = \"how does attention relate to feed forward networks?\"\n",
    "response = chat_with_documents(messages=[{\"role\": \"user\", \"content\": user_message}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the Transformer model architecture, attention mechanisms are integral to both the encoder and decoder, and work alongside feed-forward networks. Here is how they relate:\n",
       "\n",
       "1. **Structure**: The Transformer uses stacked self-attention and point-wise, fully connected layers (feed-forward networks) for both the encoder and decoder. This can be seen in the diagram from the paper, which alternates between multi-head attention layers and feed-forward layers [1].\n",
       "\n",
       "2. **Complementary Functions**:\n",
       "    - **Self-Attention**: Enables the model to consider and weigh the relevance of different positions in the input sequence for generating the output, without regard to their distance.\n",
       "    - **Feed-Forward Networks**: Applied independently to each position, adding non-linearity and enhancing the model's ability to learn complex patterns.\n",
       "\n",
       "The Transformer architecture can thus be viewed as an interplay between attention mechanisms capturing dependencies and feed-forward networks processing these representations locally.\n",
       "\n",
       "### Reference:\n",
       "[1] \"The Transformer - model architecture,\" papers/1706.03762v7.pdf. [Online]. Available: https://dataericssonlearningpath.blob.core.windows.net/demo-data/papers/1706.03762v7.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ericsson-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
