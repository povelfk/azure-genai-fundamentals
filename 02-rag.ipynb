{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retrieval-Augmented Generation (RAG) with Azure AI Foundry**\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to implement Retrieval-Augmented Generation (RAG) using Azure AI Foundry services. You'll learn how to enhance AI model responses with relevant information retrieved from your own data sources, improving accuracy and context-awareness of the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "Retrieval-Augmented Generation combines the strengths of two approaches:\n",
    "\n",
    "1. **Retrieval**: Finding relevant information from a knowledge base or vector store\n",
    "2. **Generation**: Using this retrieved context to generate accurate, informed responses\n",
    "\n",
    "This hybrid approach helps solve several limitations of standalone generative models:\n",
    "- Provides access to specialized knowledge not in the model's training data\n",
    "- Reduces hallucinations by grounding responses in factual information\n",
    "- Allows for real-time access to updated information\n",
    "- Makes citation and attribution of sources possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, we'll load environment variables and set up OpenTelemetry for tracing our requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\", override=True)\n",
    "\n",
    "from opentelemetry import trace\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Azure AI Foundry Clients\n",
    "\n",
    "Now we'll set up connections to essential services:\n",
    "- AI Project Client to manage project resources\n",
    "- Chat Completions client for query understanding and response generation\n",
    "- Embeddings client for vector representation\n",
    "- Search client for document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# create a project client using environment variables loaded from the .env file\n",
    "project = AIProjectClient.from_connection_string(\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"], credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# create a vector embeddings client that will be used to generate vector embeddings\n",
    "chat = project.inference.get_chat_completions_client()\n",
    "embeddings = project.inference.get_embeddings_client()\n",
    "\n",
    "# use the project client to get the default search connection\n",
    "search_connection = project.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_AI_SEARCH, include_credentials=True\n",
    ")\n",
    "\n",
    "# Create a search index client using the search connection\n",
    "# This client will be used to create and delete search indexes\n",
    "search_client = SearchClient(\n",
    "    index_name=os.environ[\"SEARCH_INDEX_NAME\"],\n",
    "    endpoint=search_connection.endpoint_url,\n",
    "    credential=AzureKeyCredential(key=search_connection.key),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Retrieval Component\n",
    "\n",
    "The retrieval component is responsible for finding relevant documents that match the user's query. There are two main steps:\n",
    "\n",
    "1. Understanding user intent to create an optimized search query\n",
    "2. Using that optimized query to retrieve relevant documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent System Message Setup\n",
    "\n",
    "This system message instructs the model to analyze user queries and reformat them into optimized search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import UserMessage, SystemMessage\n",
    "\n",
    "# Define your INTENT_SYSTEM_PROMPT correctly with escaped braces\n",
    "INTENT_SYSTEM_PROMPT = \"\"\"\n",
    "# Semantic Intent Clarification System\n",
    "\n",
    "Your task is to rephrase the user's query into a concise, standalone phrase or sentence that clearly captures the semantic intent.  \n",
    "The output will be used for semantic similarity retrieval, so it should fully represent the user's information need.\n",
    "\n",
    "Guidelines:\n",
    "- Do not use conversational phrases (\"I want,\" \"Can you tell me,\" etc.).\n",
    "- Ensure clarity, completeness, and semantic accuracy.\n",
    "- Include related relevant concepts naturally to enhance semantic richness, if applicable.\n",
    "\n",
    "Example:\n",
    "- User Query: \"How do attention mechanisms work in transformer models?\"\n",
    "- Intent: \"Function and role of self-attention mechanisms in transformer architectures for NLP tasks\"\n",
    "\n",
    "Conversation History:\n",
    "{conversation_history}\n",
    "\n",
    "Respond strictly with the following JSON format:\n",
    "{{\"intent\": \"your clarified semantic query here\"}}\n",
    "\"\"\"\n",
    "\n",
    "# Then fix your get_intent_system_message function to properly escape the curly braces in the output\n",
    "def get_intent_system_message(conversation_history):\n",
    "    return SystemMessage(INTENT_SYSTEM_PROMPT.format(conversation_history=conversation_history)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Retrieval Function\n",
    "\n",
    "This function handles the full document retrieval process:\n",
    "1. Analyze user intent to create optimized search query\n",
    "2. Generate embeddings for the optimized query\n",
    "3. Perform a hybrid search (both text-based and vector-based)\n",
    "4. Return the most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "import json\n",
    "\n",
    "@tracer.start_as_current_span(name=\"get_documents\")\n",
    "def get_documents(messages: list, top: int=3) -> dict:\n",
    "    intent_query_response = chat.complete(\n",
    "        model=os.environ[\"chatModel\"],\n",
    "        messages=[get_intent_system_message(messages)]\n",
    "    )\n",
    "\n",
    "    enhanced_search_query = json.loads(intent_query_response.choices[0].message.content)[\"intent\"]\n",
    "    \n",
    "    embedding = embeddings.embed(\n",
    "        model=os.environ[\"embeddingModel\"],\n",
    "        input=enhanced_search_query\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=50, fields=\"text_vector\")\n",
    "\n",
    "    search_results = search_client.search(\n",
    "        search_text=enhanced_search_query,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"id\", \"content\", \"title\", \"url\"],\n",
    "        top=top,\n",
    "    )\n",
    "\n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": result[\"id\"],\n",
    "            \"content\": result[\"content\"],\n",
    "            \"title\": result[\"title\"],\n",
    "            \"url\": result[\"url\"],\n",
    "        }\n",
    "        for result in search_results\n",
    "    ]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Generation Component\n",
    "\n",
    "The generation component uses the retrieved documents as context to create accurate, well-informed responses to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG System Message Setup\n",
    "\n",
    "This system message instructs the model on how to use retrieved context to generate responses. It emphasizes:\n",
    "- Staying grounded in the provided context\n",
    "- Proper citation and attribution\n",
    "- Clear formatting with Markdown for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that provides accurate information based on the retrieved context.\n",
    "\n",
    "### Retrieved Context:\n",
    "{retrieved_context}\n",
    "\n",
    "### Instructions:\n",
    "1. Answer questions based on the retrieved context above.\n",
    "2. If the context doesn't contain the information needed, acknowledge the limitation.\n",
    "3. Do not make up information that is not supported by the context.\n",
    "4. Keep responses concise and focused on the user's question.\n",
    "5. Format your answers using Markdown when appropriate (so we can render them using from IPython.display import display, Markdown).\n",
    "    5a. pay extra attention on formatting code blocks, lists, tables, and mathematical equations.\n",
    "6. When quoting directly from the context, use quotation marks.\n",
    "7. In terms of citation style please follow the Institute for Electrical and Electronics Engineers (IEEE):\n",
    "    7a. In-text citations should be in square brackets, e.g. [1], [2], etc.\n",
    "    7b. The reference list should be numbered in the order in which they appear in the text (make them clickable).\n",
    "    7c. Use the following format for references: [1] Author(s), \"Title,\" Journal, vol. X, no. Y, pp. Z-Z, Year. [Online]. Available: URL\n",
    "\n",
    "Remember: Only use information from the retrieved context to answer questions, and remember to cite sources properly.\n",
    "\"\"\"\n",
    "\n",
    "def get_completion_system_message(retrieved_context):\n",
    "    return SystemMessage(SYSTEM_PROMPT.format(retrieved_context=retrieved_context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete RAG Pipeline Function\n",
    "\n",
    "This function integrates both retrieval and generation components:\n",
    "1. Retrieve relevant documents based on user query\n",
    "2. Format the system message with retrieved context\n",
    "3. Generate a response that incorporates the retrieved information\n",
    "4. Return the formatted response to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.start_as_current_span(name=\"chat_with_attentionIsAllYouNeed\")\n",
    "def chat_with_documents(messages: list) -> dict:\n",
    "    documents = get_documents(messages)\n",
    "\n",
    "    # Create the system message\n",
    "    system_message = get_completion_system_message(documents)\n",
    "\n",
    "    # Format messages properly for the API\n",
    "    formatted_messages = [system_message]\n",
    "\n",
    "    # Add user messages\n",
    "    for message in messages:\n",
    "        formatted_messages.append(UserMessage(message[\"content\"]))\n",
    "\n",
    "    response = chat.complete(\n",
    "        model=os.environ[\"chatModel\"],\n",
    "        messages=formatted_messages,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "\n",
    "    # Return a chat protocol compliant response\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Telemetry Integration\n",
    "\n",
    "Telemetry helps us monitor and analyze RAG system performance. This function sets up:\n",
    "- Azure Monitor integration\n",
    "- OpenTelemetry instrumentation\n",
    "- Tracing capabilities for inference operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.core.settings import settings\n",
    "\n",
    "def enable_telemetry(project):\n",
    "    AIInferenceInstrumentor().instrument()\n",
    "    settings.tracing_implementation = \"opentelemetry\"\n",
    "    application_insights_connection_string = project.telemetry.get_connection_string()\n",
    "    configure_azure_monitor(connection_string=application_insights_connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the RAG Pipeline\n",
    "\n",
    "Now let's test our complete RAG pipeline with a sample question about attention mechanisms in neural networks.\n",
    "This will demonstrate how the system:\n",
    "1. Processes the question\n",
    "2. Retrieves relevant documents\n",
    "3. Generates a well-informed response with citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from config import enable_telemetry\n",
    "enable_telemetry(project)\n",
    "\n",
    "user_message = \"On a high level, how are llms different from diffusion models?\"\n",
    "response = chat_with_documents(messages=[{\"role\": \"user\", \"content\": user_message}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the Response\n",
    "\n",
    "We'll use IPython's Markdown display capability to render the response with proper formatting, including:\n",
    "- Headers and sections\n",
    "- Code blocks (if present)\n",
    "- Citations and references\n",
    "- Mathematical equations (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large Language Models (LLMs) and diffusion models differ in their fundamental approach to processing and generating data:\n",
       "\n",
       "### 1. **Modeling Approach**\n",
       "- **LLMs**: These are predominantly **autoregressive models**, which predict the next token given the context of preceding tokens. They generate outputs sequentially at the token level, relying heavily on architectures like transformer-based, decoder-only language models [1].\n",
       "- **Diffusion Models**: These are based on **probabilistic diffusion processes**. They iteratively refine random noise over multiple steps to generate coherent outputs. Diffusion models operate in continuous embedding spaces and are an alternative probabilistic paradigm for text generation [2][3].\n",
       "\n",
       "### 2. **Focus**\n",
       "- **LLMs**: Heavily token-based and often operate at the word or subword level. Their training is generally English-centric, with a strong dependency on tokens and language-based patterns [1].\n",
       "- **Diffusion Models**: They aim for reasoning and text generation in abstract embedding spaces. This enables them to model higher-level semantic relationships and hierarchical reasoning that LLMs typically do not capture [2].\n",
       "\n",
       "### 3. **Applications**\n",
       "While both can be used for tasks like natural language processing, conversational AI, or code generation, diffusion models may offer advantages in tasks requiring complex reasoning or hierarchical abstraction due to their focus on higher-level embeddings [2].\n",
       "\n",
       "### 4. **Concerns**\n",
       "Both models share societal challenges, such as environmental impact due to large-scale training, potential biases in training data, and misuse for generating harmful content [3].\n",
       "\n",
       "### References\n",
       "[1] \"Large Language Models,\" [Online]. Available: https://dataericssonlearningpath.blob.core.windows.net/demo-data/papers/2412.08821v2.pdf  \n",
       "[2] \"Deep Probabilistic Paradigms in LLMs,\" [Online]. Available: https://dataericssonlearningpath.blob.core.windows.net/demo-data/papers/2412.08821v2.pdf  \n",
       "[3] \"Diffusion-based Alternatives,\" [Online]. Available: https://dataericssonlearningpath.blob.core.windows.net/demo-data/papers/2502.09992v2.pdf  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ericsson-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
