{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluating AI Models in Azure AI Foundry**\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to evaluate AI model outputs using Azure AI Foundry's evaluation capabilities. You'll learn how to assess the groundedness and quality of AI-generated responses, ensuring they're factually accurate and aligned with provided context.\n",
    "\n",
    "## Evaluation Types in Azure AI Foundry\n",
    "\n",
    "### **Groundedness Evaluation**\n",
    "Groundedness evaluates whether an AI model's responses are properly supported by the provided context:\n",
    "\n",
    "- **GroundednessEvaluator**: Assesses if the model's response contains claims that are verifiable within the given context\n",
    "- **GroundednessProEvaluator**: An enhanced version that provides more detailed evaluation metrics\n",
    "\n",
    "### **Other Common Evaluations (Not Covered in This Notebook)**\n",
    "- **Relevance**: Measures how well the response addresses the user query\n",
    "- **Coherence**: Evaluates the logical flow and consistency of the response\n",
    "- **Fluency**: Analyzes the grammatical correctness and readability of the text\n",
    "- **Toxicity**: Checks for harmful, offensive, or inappropriate content\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up evaluation in Azure AI Foundry\n",
    "- Configure evaluators for assessing groundedness of AI responses\n",
    "- Run evaluations on sample data\n",
    "\n",
    "## Prerequisites\n",
    "- An Azure account with access to Azure AI Foundry\n",
    "- Azure OpenAI connection configured in your Azure AI Foundry project\n",
    "- Appropriate environment variables in a `.env` file:\n",
    "  - `PROJECT_CONNECTION_STRING`: Connection string for your Azure AI Foundry project\n",
    "  - `OAI_CONNECTION_NAME`: Name of your Azure OpenAI connection\n",
    "  - `chatModel`: The model to use for evaluation (e.g., GPT-4)\n",
    "  - `AZURE_OPENAI_API_VERSION`: API version for Azure OpenAI\n",
    "\n",
    "## Workflow\n",
    "This notebook walks through the complete process of evaluating AI model outputs:\n",
    "1. Setting up the environment and client\n",
    "2. Configuring evaluators for groundedness assessment\n",
    "3. Running evaluations on sample data\n",
    "4. Uploading and analyzing evaluation results in Azure AI Foundry\n",
    "\n",
    "Follow along step by step to learn how to evaluate your AI models in Azure AI Foundry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"], credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "oai_connection = project_client.connections.get(\n",
    "    connection_name=os.getenv(\"OAI_CONNECTION_NAME\"),\n",
    "    include_credentials=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run evaluation and upload results to AI Foundry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "import datetime\n",
    "from azure.ai.projects.models import EvaluatorConfiguration, ConnectionType\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "\n",
    "GroundednessEvaluator\n",
    "\n",
    "def run_eval_on_azure(model_config, data_path):\n",
    "    now = datetime.datetime.now()\n",
    "    result = evaluate(\n",
    "        evaluation_name = f\"groundedness-{now.strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    "        data=data_path,\n",
    "        evaluators={\n",
    "            \"GroundednessEvaluator\": GroundednessEvaluator(model_config=model_config),\n",
    "            \"GroundednessProEvaluator\": GroundednessProEvaluator(azure_ai_project=project_client.scope, credential=DefaultAzureCredential()),\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"GroundednessProEvaluator\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"context\": \"${data.context}\",\n",
    "                    \"response\": \"${data.response}\"\n",
    "                }\n",
    "            },\n",
    "            \"GroundednessEvaluator\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"context\": \"${data.context}\",\n",
    "                    \"response\": \"${data.response}\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        azure_ai_project = project_client.scope,\n",
    "        # output_path=\"./myevalresults.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-04 16:52:32 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-04-04 16:52:32 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-04-04 16:52:32 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_w58dwg92_20250404_165232_024664, log path: C:\\Users\\povelf\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_w58dwg92_20250404_165232_024664\\logs.txt\n",
      "[2025-04-04 16:52:32 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_u65j6es6_20250404_165232_024664, log path: C:\\Users\\povelf\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_u65j6es6_20250404_165232_024664\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_w58dwg92_20250404_165232_024664\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_u65j6es6_20250404_165232_024664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-04 16:52:32 +0200][promptflow.core._prompty_utils][ERROR] - Exception occurs: AuthenticationError: Error code: 401 - {'error': {'code': '401', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "[2025-04-04 16:52:32 +0200][promptflow.core._prompty_utils][ERROR] - Exception occurs: AuthenticationError: Error code: 401 - {'error': {'code': '401', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "[2025-04-04 16:52:32 +0200][promptflow.core._prompty_utils][ERROR] - Exception occurs: AuthenticationError: Error code: 401 - {'error': {'code': '401', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "[2025-04-04 16:52:33 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 3 out of 3 runs failed in batch run.\n",
      " Please check out C:/Users/povelf/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_u65j6es6_20250404_165232_024664 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 16:52:48 +0200   32776 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-04-04 16:52:48 +0200   32776 execution.bulk     INFO     Average execution time for completed lines: 15.9 seconds. Estimated time for incomplete lines: 31.8 seconds.\n",
      "2025-04-04 16:52:48 +0200   32776 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-04 16:52:48 +0200   32776 execution.bulk     INFO     Average execution time for completed lines: 15.9 seconds. Estimated time for incomplete lines: 31.8 seconds.\n",
      "2025-04-04 16:52:48 +0200   32776 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-04 16:52:48 +0200   32776 execution.bulk     INFO     Average execution time for completed lines: 7.96 seconds. Estimated time for incomplete lines: 7.96 seconds.\n",
      "2025-04-04 16:52:48 +0200   32776 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-04 16:52:48 +0200   32776 execution.bulk     INFO     Average execution time for completed lines: 5.34 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/eval.jsonl\"\n",
    "\n",
    "######## get configs ########\n",
    "model_config = oai_connection.to_evaluator_model_config(\n",
    "    deployment_name=os.environ.get(\"chatModel\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "######## run evaluation ########\n",
    "run_eval_on_azure(\n",
    "    model_config,\n",
    "    data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ericsson-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
