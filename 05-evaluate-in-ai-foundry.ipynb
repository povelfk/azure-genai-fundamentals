{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluating AI Models in Azure AI Foundry**\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to evaluate AI model outputs using Azure AI Foundry's evaluation capabilities. You'll learn how to assess the groundedness and quality of AI-generated responses, ensuring they're factually accurate and aligned with provided context.\n",
    "\n",
    "## Evaluation Types in this notebook\n",
    "\n",
    "### **Groundedness Evaluation**\n",
    "Groundedness evaluates whether an AI model's responses are properly supported by the provided context:\n",
    "\n",
    "- **GroundednessEvaluator**: Measures how well the generated response aligns with the given context, focusing on its relevance and accuracy with respect to the context.\n",
    "- **GroundednessProEvaluator**: It detects whether the generated text response is consistent or accurate with respect to the given context.\n",
    "\n",
    "Read more here: [docs](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#ai-assisted-groundedness)\n",
    "\n",
    "### **Other Common Evaluations (Not Covered in This Notebook)**\n",
    "- **Relevance**: It measures how effectively a response addresses a query. It assesses the accuracy, completeness, and direct relevance of the response based solely on the given query.\n",
    "- **Coherence**: It measures the logical flow and organization of ideas in a response, allowing the reader to easily follow and understand the writer's train of thought.\n",
    "- **Fluency**: It measures the effectiveness and clarity of written communication, focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, and overall readability.\n",
    "\n",
    "Read more here: [docs](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, we'll import the necessary libraries and load environment variables from a `.env` file.\n",
    "This provides access to our Azure AI Foundry project connection string and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Azure AI Foundry Client and Connections\n",
    "\n",
    "Now we'll establish connections to our Azure AI Foundry project and Azure OpenAI service. These connections are essential for:\n",
    "\n",
    "1. Accessing evaluation capabilities in Azure AI Foundry\n",
    "2. Configuring the evaluator model that will assess our AI outputs\n",
    "3. Converting our OpenAI connection into a format compatible with evaluators\n",
    "\n",
    "The `to_evaluator_model_config()` method extracts the necessary configuration details (endpoint, key, model name) from our Azure OpenAI connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "\n",
    "oai_connection = project_client.connections.get(\n",
    "    connection_name=os.getenv(\"OAI_CONNECTION_NAME\"),\n",
    "    include_credentials=True\n",
    ")\n",
    "\n",
    "model_config = oai_connection.to_evaluator_model_config(\n",
    "    deployment_name=os.environ.get(\"chatModel\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    include_credentials=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining Evaluation Function\n",
    "\n",
    "Here we define a function that will run our evaluations and upload the results to Azure AI Foundry. This function configures:\n",
    "\n",
    "### Evaluation Types\n",
    "1. **Regular Groundedness**: Measures how well the generated response aligns with the given context, focusing on its relevance and accuracy with respect to the context.\n",
    "2. **Pro Groundedness**: It detects whether the generated text response is consistent or accurate with respect to the given context.\n",
    "\n",
    "### Data Mapping\n",
    "The `column_mapping` parameter specifies how data fields in our JSONL file map to evaluator inputs:\n",
    "- `query`: The original user question\n",
    "- `context`: The reference information that responses should be based on\n",
    "- `response`: The AI-generated answer we want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:azure.monitor.opentelemetry.exporter.export._base:Non-retryable server side error: Operation returned an invalid status 'Bad Request'.\n",
      "ERROR:azure.monitor.opentelemetry.exporter.export._base:Non-retryable server side error: Operation returned an invalid status 'Bad Request'.\n",
      "ERROR:azure.monitor.opentelemetry.exporter.export._base:Non-retryable server side error: Operation returned an invalid status 'Bad Request'.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "import datetime\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "\n",
    "def run_eval_on_azure(model_config, data_path):\n",
    "    now = datetime.datetime.now()\n",
    "    result = evaluate(\n",
    "        evaluation_name = f\"eval-demo-groundedness-{now.strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    "        data=data_path,\n",
    "        evaluators={\n",
    "            \"Regular\": GroundednessEvaluator(model_config=model_config),\n",
    "            \"Pro\": GroundednessProEvaluator(azure_ai_project=project_client.scope, credential=DefaultAzureCredential()),\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"Regular\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"context\": \"${data.context}\",\n",
    "                    \"response\": \"${data.response}\"\n",
    "                }\n",
    "            },\n",
    "            \"Pro\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"context\": \"${data.context}\",\n",
    "                    \"response\": \"${data.response}\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        azure_ai_project = project_client.scope,\n",
    "        # output_path=\"./myevalresults.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the Evaluation\n",
    "\n",
    "Now we'll execute the evaluation on our sample data file. The `eval.jsonl` file contains test cases with:\n",
    "- User queries\n",
    "- Context information\n",
    "- AI-generated responses to evaluate\n",
    "\n",
    "When this cell runs, it will:\n",
    "1. Upload the evaluation configuration to Azure AI Foundry\n",
    "2. Process each sample in the data file\n",
    "3. Score each response based on factual accuracy/groundedness\n",
    "4. Generate evaluation results viewable in the Azure AI Foundry web interface\n",
    "\n",
    "After running this cell, you can view the evaluation results in your Azure AI Foundry project portal under the 'Evaluations' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class GroundednessProEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "[2025-04-07 23:27:58 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-04-07 23:27:58 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-04-07 23:27:58 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_2khp12i_20250407_232757_355647, log path: C:\\Users\\povelf\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_2khp12i_20250407_232757_355647\\logs.txt\n",
      "[2025-04-07 23:27:58 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_e315jen1_20250407_232757_355647, log path: C:\\Users\\povelf\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_e315jen1_20250407_232757_355647\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_e315jen1_20250407_232757_355647\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_2khp12i_20250407_232757_355647\n",
      "2025-04-07 23:27:58 +0200   34460 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-04-07 23:28:00 +0200   34460 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-04-07 23:28:00 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 1.98 seconds. Estimated time for incomplete lines: 3.96 seconds.\n",
      "2025-04-07 23:28:00 +0200   34460 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-07 23:28:00 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 1.04 seconds. Estimated time for incomplete lines: 1.04 seconds.\n",
      "2025-04-07 23:28:00 +0200   34460 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-07 23:28:00 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 0.73 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_2khp12i_20250407_232757_355647\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-04-07 23:27:57.355647+02:00\"\n",
      "Duration: \"0:00:04.035492\"\n",
      "Output path: \"C:\\Users\\povelf\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_2khp12i_20250407_232757_355647\"\n",
      "\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 12.13 seconds. Estimated time for incomplete lines: 24.26 seconds.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 6.07 seconds. Estimated time for incomplete lines: 6.07 seconds.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 4.13 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-04-07 23:27:58 +0200   34460 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 12.13 seconds. Estimated time for incomplete lines: 24.26 seconds.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 6.07 seconds. Estimated time for incomplete lines: 6.07 seconds.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-07 23:28:10 +0200   34460 execution.bulk     INFO     Average execution time for completed lines: 4.13 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_e315jen1_20250407_232757_355647\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-04-07 23:27:57.355647+02:00\"\n",
      "Duration: \"0:00:13.721957\"\n",
      "Output path: \"C:\\Users\\povelf\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_e315jen1_20250407_232757_355647\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error INTERNAL SERVER ERROR encountered while exporting span batch, retrying in 1s.\n",
      "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error INTERNAL SERVER ERROR encountered while exporting span batch, retrying in 2s.\n",
      "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error INTERNAL SERVER ERROR encountered while exporting span batch, retrying in 4s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"Regular\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:04.035492\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\povelf\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_2khp12i_20250407_232757_355647\"\n",
      "    },\n",
      "    \"Pro\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:13.721957\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\povelf\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_e315jen1_20250407_232757_355647\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:opencensus.ext.azure.common.transport:Non-retryable server side error 400: {\"itemsReceived\":8,\"itemsAccepted\":0,\"appId\":null,\"errors\":[{\"index\":0,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":1,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":2,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":3,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":4,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":5,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":6,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":7,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"}]}.\n",
      "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error INTERNAL SERVER ERROR encountered while exporting span batch, retrying in 8s.\n",
      "ERROR:opencensus.ext.azure.common.transport:Non-retryable server side error 400: {\"itemsReceived\":4,\"itemsAccepted\":0,\"appId\":null,\"errors\":[{\"index\":0,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":1,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":2,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"},{\"index\":3,\"statusCode\":400,\"message\":\"Invalid instrumentation key\"}]}.\n",
      "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error INTERNAL SERVER ERROR encountered while exporting span batch, retrying in 16s.\n",
      "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error INTERNAL SERVER ERROR encountered while exporting span batch, retrying in 32s.\n"
     ]
    }
   ],
   "source": [
    "run_eval_on_azure(\n",
    "    model_config=model_config,\n",
    "    data_path=\"data/eval.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ericsson-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
